{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3 Linear Classification Models: Task 1\n",
    "_Linear Classification models using haberman data set with the Perceptron model.\n",
    "Recommended: Press the fast forward symbol to run the whole notebook so that the imports and methods are fully run before running the experiments below._\n",
    "##### Authors: Miss Katrina Jones and Dr. Aniko Ekart (ML Module, 2021)\n",
    "_PLEASE NOTE: Try to avoid copying and pasting this code or text in your portfolio submissions; try to complete the task on your own, and compare the solution to your own. Use this as reference if you encounter any issues in completing the task by yourself, and let us know if you have any queries or spot any issues with this code._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general modules\n",
    "import pandas as pd # For reading the csv\n",
    "from sklearn.linear_model import Perceptron # Our chosen model\n",
    "from sklearn.metrics import confusion_matrix # For creation of the confusion matrix\n",
    "from sklearn.metrics import classification_report # For creation of precision, recall and f1-measures\n",
    "from sklearn.metrics import accuracy_score # For help in comparing data given using accuracy score vs confusion matrix\n",
    "import warnings # Allows better control of warning messages\n",
    "\n",
    "# Set various display options\n",
    "pd.set_option('display.max_row', 20) # This is just so that we're not printing every single row in one go.\n",
    "pd.set_option('display.max_columns', 5) # If we were to expand the dataset, this is so that we're not printing every single column present.\n",
    "warnings.simplefilter('ignore') # So that we can ignore depreciation issues in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Setting up and reading the database from the csv file._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['Age', 'YearOfOp', '+AuxillaryNodes']\n",
    "classes = ['SurvivalStatus']\n",
    "\n",
    "hdata = pd.read_csv(\"haberman.csv\", names=features+classes)\n",
    "hdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>YearOfOp</th>\n",
       "      <th>+AuxillaryNodes</th>\n",
       "      <th>SurvivalStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  YearOfOp  +AuxillaryNodes  SurvivalStatus\n",
       "0   30        64                1               1\n",
       "1   30        62                3               1\n",
       "2   30        65                0               1\n",
       "3   31        59                2               1\n",
       "4   31        65                4               1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>YearOfOp</th>\n",
       "      <th>+AuxillaryNodes</th>\n",
       "      <th>SurvivalStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.457516</td>\n",
       "      <td>62.852941</td>\n",
       "      <td>4.026144</td>\n",
       "      <td>1.264706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.803452</td>\n",
       "      <td>3.249405</td>\n",
       "      <td>7.189654</td>\n",
       "      <td>0.441899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.750000</td>\n",
       "      <td>65.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age    YearOfOp  +AuxillaryNodes  SurvivalStatus\n",
       "count  306.000000  306.000000       306.000000      306.000000\n",
       "mean    52.457516   62.852941         4.026144        1.264706\n",
       "std     10.803452    3.249405         7.189654        0.441899\n",
       "min     30.000000   58.000000         0.000000        1.000000\n",
       "25%     44.000000   60.000000         0.000000        1.000000\n",
       "50%     52.000000   63.000000         1.000000        1.000000\n",
       "75%     60.750000   65.750000         4.000000        2.000000\n",
       "max     83.000000   69.000000        52.000000        2.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdata.describe() # Useful in this context due to us having so much data - we can see the range of the values in each columns..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing the Data: Grouping the data using the Year of Operation for the train-test split criteria\n",
    "_We were asked to split the data so that there were 229 rows for the train set using the years 1958-1965, and 77 rows for the test data, using the years 1966-1969._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the train set - so the data that resides between 1958-1965 (229 rows)\n",
    "hdatatrain = hdata.loc[(hdata['YearOfOp'] <= 65) & (hdata['YearOfOp'] >= 58)] # I chose to use split using boolean_indexing, but Pandas groupby is a good alternative\n",
    "hdatatrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the test set - so the data that resides above 1965 (77 rows)\n",
    "hdatatest = hdata.loc[(hdata['YearOfOp'] <= 69) & (hdata['YearOfOp'] >= 66)] # I chose to use split using boolean_indexing, but Pandas groupby is a good alternative\n",
    "hdatatest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Train and Test datasets\n",
    "_Divide the dataset into training and testing sets, by using the data for years. 1958-1965 (229 instances) for training and 1966-1969 (77 instances) for testing. Now that we have split the data in such a way, we can do this simply:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our train set...\n",
    "x_train = hdatatrain[features].values.reshape(-1, len(features))\n",
    "y_train = hdatatrain[classes]\n",
    "\n",
    "# Our test set...\n",
    "x_test = hdatatest[features].values.reshape(-1, len(features))\n",
    "y_test = hdatatest[classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Perceptron \n",
    "_Apply a single perceptron using sklearn.linear_model.Perceptron._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron() # Initialise the model we are using\n",
    "model = p.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Confusion Matrix\n",
    "_Use sklearn.metrics.confusion_matrix to obtain the confusion matrix of your classifier._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56  4]\n",
      " [13  4]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Point #1: Confusion Matrix\n",
    "- What is a confusion matrix? \n",
    "- What does the number 56 mean?\n",
    "- What does the number on the top left mean?\n",
    "- How many did it get correct?\n",
    "- Could you explain what these results are showing us?\n",
    "- Is this good... or bad..? How can you tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' NOTE: This is different to the lecture formatting of a confusion matrix, be careful! '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is taken from the Python documentation, ravel() is a useful function for retrieving these values separately.\n",
    "truenegatives, falsepositives, falsenegatives, truepositives = confusion_matrix(y_test, y_pred).ravel()\n",
    "''' NOTE: This is different to the lecture formatting of a confusion matrix, be careful! '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:56 || FP: 4 || FN: 13 || TP:4\n"
     ]
    }
   ],
   "source": [
    "# This is just to clearly illustrate what they are.\n",
    "print('TN:' + str(truenegatives), '|| FP: ' + str(falsepositives), '|| FN: ' + str(falsenegatives), '|| TP:' + str(truepositives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0           56            4\n",
       "Actual 1           13            4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is in case you wish to format the confusion matrix to have specific headings.\n",
    "rowlabels = [\"Actual 0\", \"Actual 1\"]\n",
    "columnlabels = [\"Predicted 0\", \"Predicted 1\"]\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred), rowlabels, columnlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Classification Report\n",
    "\n",
    "_Use sklearn.metrics.classification_report to print out a report on precision, recall, f1-measure for both training and test data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.93      0.87        60\n",
      "           2       0.50      0.24      0.32        17\n",
      "\n",
      "    accuracy                           0.78        77\n",
      "   macro avg       0.66      0.58      0.59        77\n",
      "weighted avg       0.74      0.78      0.75        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Point #2: Classification Report\n",
    "- What is precision?\n",
    "- What is recall?\n",
    "- What is an f1-score?\n",
    "- What is 'support'?\n",
    "- What is accuracy?\n",
    "- What is the macro average?\n",
    "- What is the weighted average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.91      0.85       165\n",
      "           2       0.64      0.42      0.51        64\n",
      "\n",
      "    accuracy                           0.77       229\n",
      "   macro avg       0.72      0.67      0.68       229\n",
      "weighted avg       0.76      0.77      0.76       229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict(x_train)\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Point #3: Comparing Classification Reports\n",
    "- Is this a good model?\n",
    "- Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with Different Parameter Settings (in a Systematic Way) \n",
    "_Note: Your experiments may be different to mine._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A helper function to help keep the experiment simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A helper function that just applies the data to the new model and prints out the output'''\n",
    "def create_and_change_defaults_of_perceptron(penaltyvalue, maxitervalue):\n",
    "    \n",
    "    # Create our local variable...\n",
    "    parametersapplied = 'N/A'\n",
    "    \n",
    "    # Check what the programmer has input, and make the Perceptron associated to it.\n",
    "    if penaltyvalue is None and maxitervalue is None:\n",
    "        percept = Perceptron()\n",
    "    elif penaltyvalue is not None and maxitervalue is None:\n",
    "        percept = Perceptron(penalty=penaltyvalue)\n",
    "        parametersapplied = 'Penalty type: ' + penaltyvalue\n",
    "    elif penaltyvalue is None and maxitervalue is not None:\n",
    "        percept = Perceptron(max_iter=maxitervalue)\n",
    "        parametersapplied = 'Maximum number of iteration(s): ' + str(maxitervalue)\n",
    "    else:\n",
    "        percept = Perceptron(penalty=penaltyvalue, max_iter=maxitervalue)\n",
    "        parametersapplied = 'Penalty type: ' + penaltyvalue + ' Maximum number of iteration(s): ' + str(maxitervalue)\n",
    "    \n",
    "    # Apply the data to the model...\n",
    "    emodel = percept.fit(x_train, y_train)\n",
    "    ey_pred = emodel.predict(x_test)\n",
    "    # Create a confusion matrix...\n",
    "    cm = confusion_matrix(y_test, ey_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    ttpc = tn + tp\n",
    "    ttpi = fp + fn\n",
    "    \n",
    "    # Print out statistics about train-test data split and amount of data used (will help us to understand the confusion matrix).\n",
    "    print(\"Total amount of data (test and train): %d\" % len(hdata))\n",
    "    print(\"Total amount of training data: %d\" % len(x_train))\n",
    "    print(\"Total amount of testing data: %d \\n\" % len(x_test))\n",
    "    print(\"The aribtary accuracy score: %.2f\" % accuracy_score(y_test, ey_pred))\n",
    "    # Print out our findings...\n",
    "    print(\"The confusion matrix with a \" + parametersapplied + \" applied: \\n\")\n",
    "    print(cm)\n",
    "    print('TN: ' + str(tn), '|| FP: ' + str(fp), '|| FN: ' + str(fn), '|| TP: ' + str(tp))\n",
    "    print('Total Number predicted correctly (true negatives/positives): ' + str(ttpc))\n",
    "    print('Total Number predicted incorrectly: (false negatives/positives): ' + str(ttpi))\n",
    "    print(\"\\n --------------------- \\n Classification Report: \\n\")\n",
    "    # Print out our classification report...\n",
    "    print(classification_report(y_test, ey_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Part 1: Applying Different Types of Penalties to Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our default Perceptron, just for quick reference..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data (test and train): 306\n",
      "Total amount of training data: 229\n",
      "Total amount of testing data: 77 \n",
      "\n",
      "The aribtary accuracy score: 0.78\n",
      "The confusion matrix with a N/A applied: \n",
      "\n",
      "[[56  4]\n",
      " [13  4]]\n",
      "TN: 56 || FP: 4 || FN: 13 || TP: 4\n",
      "Total Number predicted correctly (true negatives/positives): 60\n",
      "Total Number predicted incorrectly: (false negatives/positives): 17\n",
      "\n",
      " --------------------- \n",
      " Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.93      0.87        60\n",
      "           2       0.50      0.24      0.32        17\n",
      "\n",
      "    accuracy                           0.78        77\n",
      "   macro avg       0.66      0.58      0.59        77\n",
      "weighted avg       0.74      0.78      0.75        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_change_defaults_of_perceptron(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data (test and train): 306\n",
      "Total amount of training data: 229\n",
      "Total amount of testing data: 77 \n",
      "\n",
      "The aribtary accuracy score: 0.51\n",
      "The confusion matrix with a Penalty type: l2 applied: \n",
      "\n",
      "[[32 28]\n",
      " [10  7]]\n",
      "TN: 32 || FP: 28 || FN: 10 || TP: 7\n",
      "Total Number predicted correctly (true negatives/positives): 39\n",
      "Total Number predicted incorrectly: (false negatives/positives): 38\n",
      "\n",
      " --------------------- \n",
      " Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.53      0.63        60\n",
      "           2       0.20      0.41      0.27        17\n",
      "\n",
      "    accuracy                           0.51        77\n",
      "   macro avg       0.48      0.47      0.45        77\n",
      "weighted avg       0.64      0.51      0.55        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_change_defaults_of_perceptron('l2', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data (test and train): 306\n",
      "Total amount of training data: 229\n",
      "Total amount of testing data: 77 \n",
      "\n",
      "The aribtary accuracy score: 0.78\n",
      "The confusion matrix with a Penalty type: l1 applied: \n",
      "\n",
      "[[56  4]\n",
      " [13  4]]\n",
      "TN: 56 || FP: 4 || FN: 13 || TP: 4\n",
      "Total Number predicted correctly (true negatives/positives): 60\n",
      "Total Number predicted incorrectly: (false negatives/positives): 17\n",
      "\n",
      " --------------------- \n",
      " Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.93      0.87        60\n",
      "           2       0.50      0.24      0.32        17\n",
      "\n",
      "    accuracy                           0.78        77\n",
      "   macro avg       0.66      0.58      0.59        77\n",
      "weighted avg       0.74      0.78      0.75        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_change_defaults_of_perceptron('l1', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data (test and train): 306\n",
      "Total amount of training data: 229\n",
      "Total amount of testing data: 77 \n",
      "\n",
      "The aribtary accuracy score: 0.51\n",
      "The confusion matrix with a Penalty type: elasticnet applied: \n",
      "\n",
      "[[32 28]\n",
      " [10  7]]\n",
      "TN: 32 || FP: 28 || FN: 10 || TP: 7\n",
      "Total Number predicted correctly (true negatives/positives): 39\n",
      "Total Number predicted incorrectly: (false negatives/positives): 38\n",
      "\n",
      " --------------------- \n",
      " Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.53      0.63        60\n",
      "           2       0.20      0.41      0.27        17\n",
      "\n",
      "    accuracy                           0.51        77\n",
      "   macro avg       0.48      0.47      0.45        77\n",
      "weighted avg       0.64      0.51      0.55        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_change_defaults_of_perceptron('elasticnet', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data (test and train): 306\n",
      "Total amount of training data: 229\n",
      "Total amount of testing data: 77 \n",
      "\n",
      "The aribtary accuracy score: 0.78\n",
      "The confusion matrix with a Maximum number of iteration(s): 1 applied: \n",
      "\n",
      "[[60  0]\n",
      " [17  0]]\n",
      "TN: 60 || FP: 0 || FN: 17 || TP: 0\n",
      "Total Number predicted correctly (true negatives/positives): 60\n",
      "Total Number predicted incorrectly: (false negatives/positives): 17\n",
      "\n",
      " --------------------- \n",
      " Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      1.00      0.88        60\n",
      "           2       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.78        77\n",
      "   macro avg       0.39      0.50      0.44        77\n",
      "weighted avg       0.61      0.78      0.68        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_change_defaults_of_perceptron(None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data (test and train): 306\n",
      "Total amount of training data: 229\n",
      "Total amount of testing data: 77 \n",
      "\n",
      "The aribtary accuracy score: 0.78\n",
      "The confusion matrix with a Maximum number of iteration(s): 50 applied: \n",
      "\n",
      "[[56  4]\n",
      " [13  4]]\n",
      "TN: 56 || FP: 4 || FN: 13 || TP: 4\n",
      "Total Number predicted correctly (true negatives/positives): 60\n",
      "Total Number predicted incorrectly: (false negatives/positives): 17\n",
      "\n",
      " --------------------- \n",
      " Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.93      0.87        60\n",
      "           2       0.50      0.24      0.32        17\n",
      "\n",
      "    accuracy                           0.78        77\n",
      "   macro avg       0.66      0.58      0.59        77\n",
      "weighted avg       0.74      0.78      0.75        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_change_defaults_of_perceptron(None, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data (test and train): 306\n",
      "Total amount of training data: 229\n",
      "Total amount of testing data: 77 \n",
      "\n",
      "The aribtary accuracy score: 0.78\n",
      "The confusion matrix with a Penalty type: l1 Maximum number of iteration(s): 1 applied: \n",
      "\n",
      "[[60  0]\n",
      " [17  0]]\n",
      "TN: 60 || FP: 0 || FN: 17 || TP: 0\n",
      "Total Number predicted correctly (true negatives/positives): 60\n",
      "Total Number predicted incorrectly: (false negatives/positives): 17\n",
      "\n",
      " --------------------- \n",
      " Classification Report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      1.00      0.88        60\n",
      "           2       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.78        77\n",
      "   macro avg       0.39      0.50      0.44        77\n",
      "weighted avg       0.61      0.78      0.68        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_and_change_defaults_of_perceptron('l1', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of Task Discussion: Are the results as you expected? How can you explain them?\n",
    "\n",
    "_For each solution, compare the results on training and testing data..._\n",
    "\n",
    "- What are the values of a default Perceptron?\n",
    "- What are the differences?\n",
    "- Can you explain them?\n",
    "- How did you decide which model is better?\n",
    "- What parameters led to the best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
