{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pratical Task 8: Reinforcement Learning :: Grid World\n",
    "\n",
    "### Learning Outcomes:\n",
    "In the current unit, we studied Markov Decision Processes and methods for evaluating policies on them. In this lab, we will put our knowledge into practice using Python.\n",
    "\n",
    "### Instructions:\n",
    "Attached to the lab task on Blackboard is a file grid_world.py. This is an implementation of the grid world MDP discussed in lectures. You will be working with the code during this lab.\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0: Familiarise yourself with Grid World\n",
    "    \n",
    "To get started, familiarise yourself with the code in grid_world.py (included in the cells between these dividers for the JupyterNotebook version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Examine the imported modules...\n",
    "\n",
    "We need to understand what 'enum' and 'random' is, first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum # Helps us to create classes.\n",
    "import random # Create a random number using this import."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Examine the premise of Grid World and determine what the code represents...\n",
    "\n",
    "We need to understand what the purpose behind Grid World is before we can actually understand what the code is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examination of Code: Welcome to Grid World!\n",
    "\n",
    "First of all, let's set the setting - imagine a 3x4 grid, that looks like this:\n",
    "\n",
    "|x|x|x|x| <br>\n",
    "|x|x|x|x| <br>\n",
    "|x|x|x|x|\n",
    "\n",
    "Each 'x' on the grid is referred to as a possible 'state' that the robot can be in when in the game.\n",
    "\n",
    "Instead of xs, we refer to each 'valid state' by their position with specific co-ordinates:\n",
    "\n",
    "|(1,3)|(2,3)|(3,3)|(4,3)| <br>\n",
    "|(1,2)|(2,2)|(3,2)|(4,2)| <br>\n",
    "|(1,1)|(2,1)|(3,1)|(4,1)|\n",
    "\n",
    "'Invalid states' are any states that would cause the robot to move outside of the grid or states that have been purposefully blocked off - as the robot can't fall off the map nor can it walk through walls.\n",
    "\n",
    "In this version of Grid World, you can think of yourself as a programmer who instructs the robot in how to navigate through the grid to reach something called a 'terminal state' - a state that ends the game. Ideally, the robot wants to achieve its goal - to reach a terminal state with a positive reward (+1) in which the robot 'wins', instead of a neutral terminal state (with 0 reward) or a negative reward (-1) in which the robot 'loses'. \n",
    "\n",
    "The robot is placed in a random 'starting state' - a state the robot begins its journey in - and it repeatedly assesses the available movements (up, down, left and right) to determine which movement will get it closer to its goal (that magical +1 reward). Movements are known as 'actions', and how the robot chooses to prioritise actions is known as its 'policy'.\n",
    "\n",
    "Let's redraw the grid to mark which positions are valid states (S), blockages (X), terminal states (T) and the rewards (T(+1), T(-1)):\n",
    "\n",
    "|S|S|S|T(+1)| <br>\n",
    "|S|X|S|T(-1)| <br>\n",
    "|S|S|S|S|\n",
    "\n",
    "The robot can be created in any of the S positions, and needs to get to the T(+1) position to 'win' while avoiding T(-1) to avoid losing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified code from grid_world.py (by Dr. Harry Goldingay)\n",
    "\n",
    "''' Helper function to check if state is terminal, i.e. whether the game should end if the robot falls on it. '''\n",
    "def _is_terminal(pos):\n",
    "    # [4,3] is our win condition, [4,2] is our loss condition.\n",
    "    return pos == [4,3] or pos == [4,2]\n",
    "\n",
    "\n",
    "def _stochastic_action(chosen_action):\n",
    "    true_action = chosen_action\n",
    "    if chosen_action == Move.up or chosen_action == Move.down:\n",
    "        r = random.randint(0,9)\n",
    "        if r == 0:\n",
    "            true_action = Move.right\n",
    "        elif r == 1:\n",
    "            true_action = Move.left\n",
    "    if chosen_action == Move.right or chosen_action == Move.left:\n",
    "        r = random.randint(0,9)\n",
    "        if r == 0:\n",
    "            true_action = Move.up\n",
    "        elif r == 1:\n",
    "            true_action = Move.down\n",
    "    return true_action\n",
    "\n",
    "''' Make sure the move is not out of bounds so for (x, y) ensure x is between 1 and 4, and y is between 1 and 3. '''\n",
    "def _bounded_move(frm,to):\n",
    "    \n",
    "    # if x is less than 1, it is out of bounds.\n",
    "    if to[0] < 1:\n",
    "        to[0] = 1\n",
    "    # if x is greater than 4, it is out of bounds.\n",
    "    elif to[0] > 4:\n",
    "        to[0] = 4\n",
    "    # if y is less than 1, it is out of bounds.\n",
    "    elif to[1] < 1:\n",
    "        to[1] = 1\n",
    "    # if y is greater than 3, it is out of bounds.\n",
    "    elif to[1] > 3:\n",
    "        to[1] = 3\n",
    "    # if [2,2] then a blockage/wall is at that position so the robot stays put.\n",
    "    elif to == [2,2]:\n",
    "        to = frm \n",
    "    return to\n",
    "\n",
    "''' Make the robot move - it needs the position it is in prior to the move, \n",
    "and the chosen action - i.e., Move.up, Move.down, Move.left, Move.right.'''\n",
    "def _move(pos, chosen_action, step):\n",
    "    \n",
    "    #_move should not be called in terminal states\n",
    "    assert not _is_terminal(pos)\n",
    "\n",
    "    #action modified with probability 0.2\n",
    "    true_action = _stochastic_action(chosen_action)\n",
    "    \n",
    "    #make a candidate move\n",
    "    if true_action == Move.up:\n",
    "        candidate_pos = [pos[0],pos[1]+1]\n",
    "    elif true_action == Move.right:\n",
    "        candidate_pos = [pos[0]+1,pos[1]]\n",
    "    elif true_action == Move.down:\n",
    "        candidate_pos = [pos[0],pos[1]-1]\n",
    "    elif true_action == Move.left:\n",
    "        candidate_pos = [pos[0]-1,pos[1]]\n",
    "\n",
    "    #prevent the candidate move from moving out of bounds\n",
    "    next_pos = _bounded_move(pos, candidate_pos)\n",
    "\n",
    "    #return the new position and the reward for the current state\n",
    "    non_terminal_reward = -0.04\n",
    "    \n",
    "    if candidate_pos != next_pos or next_pos == pos:\n",
    "        if candidate_pos == [2,2]:\n",
    "            print(\"[Step: \" + str(step) + \"] Robot attempted to \" + str(true_action) + \" from \" + str(pos) + \" to \" + str(candidate_pos) + \" but failed as a wall was in the way.\")\n",
    "        else:\n",
    "            print(\"[Step: \" + str(step) + \"] Robot attempted to \" + str(true_action) + \" from \" + str(pos) + \" to an out of bounds/illegal position and failed.\")        \n",
    "    else:\n",
    "        print(\"[Step: \" + str(step) + \"] Robot attempted to  \" + str(true_action) + \" from \" + str(pos) + \" to \" + str(next_pos) + \" and succeeded.\")\n",
    "    \n",
    "    step = step + 1\n",
    "    \n",
    "    return (next_pos, non_terminal_reward)\n",
    "\n",
    "def _terminal_reward(pos):\n",
    "    #terminal reward should only be called in a terminal state\n",
    "    assert _is_terminal(pos)\n",
    "    \n",
    "    print(\"The robot landed in the terminal state of: \" + str(pos))\n",
    "    print(\"\\n============================== \\n Grid World game has ended... \\n============================== \\n\\nResults: \\n\")\n",
    "\n",
    "    if pos == [4,3]:\n",
    "        print(\"The robot won, congratulations!\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"The robot lost...\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifiable Coding from grid_world.py by Harry Goldingay\n",
    "\n",
    "''' Just a class that assigns numbers to movements - they could be replaced with any number, but this is just used \n",
    "to differentiate between movements. '''\n",
    "class Move(enum.Enum):\n",
    "    up = 0\n",
    "    right = 1\n",
    "    down = 2\n",
    "    left = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifiable Coding from grid_world.py by Harry Goldingay - extension made by Katrina Jones to print out grid and text.\n",
    "\n",
    "def run_policy(start_pos, policy):\n",
    "    positions = []\n",
    "    rewards = []\n",
    "    step = 1\n",
    "\n",
    "    #while non-terminal, move according to the policy, recording positions and rewards.\n",
    "    current_pos = start_pos\n",
    "    \n",
    "    # Explain the game is starting to the user...\n",
    "    print(\"[0] Randomly generated starting position for robot is... \" + str(start_pos) + \"\\n\")\n",
    "    print(print_grid(start_pos))\n",
    "    \n",
    "    # While the terminal state has not been reached...\n",
    "    while not _is_terminal(current_pos):\n",
    "        # Record the position...\n",
    "        positions.append(current_pos)\n",
    "        action = policy(current_pos)\n",
    "        # Move the robot...\n",
    "        (current_pos,reward) = _move(current_pos, action, step)\n",
    "        # Increment the number of steps taken...\n",
    "        step = step + 1\n",
    "        # Record the rewards.\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    #record the final terminal postion and reward\n",
    "    positions.append(current_pos)\n",
    "    rewards.append(_terminal_reward(current_pos))\n",
    "    print(\"Your robot worked out a path to the terminal state in \" + str(step-1) + \" moves. \\nWant to try again? \\n\\nHere are the positions and rewards that the robot attained along the way: \\n\")\n",
    "    print(\"Positions: \" + str(positions) + \" || Rewards: \" + str(rewards) + \"\\n\\n\") \n",
    "    return positions, rewards\n",
    "\n",
    "''' Function that just prints out the starting position the robot and the grid layout. '''\n",
    "def print_grid(start_pos):\n",
    "    \n",
    "    row1 = \"|(1,3)|(2,3)|(3,3)|WIN+1|\"\n",
    "    row2 = \"|(1,2)|(WALL)|(3,2)|LOSS-1|\"\n",
    "    row3 = \"|(1,1)|(2,1)|(3,1)|(4,1)|\"\n",
    "    \n",
    "    if start_pos[0] == 1:\n",
    "        if start_pos[1] == 1:\n",
    "            row3 = \"|(BOT)|(2,1)|(3,1)|(4,1)|\" \n",
    "        elif start_pos[1] == 2:\n",
    "            row2 = \"|(BOT)|(WALL)|(3,2)|LOSS-1|\"\n",
    "        elif start_pos[1] == 3:\n",
    "            row1 = \"|(BOT)|(2,3)|(3,3)|WIN+1|\"\n",
    "    if start_pos[0] == 2:\n",
    "        if start_pos[1] == 1:\n",
    "            row3 = \"|(1,1)|(BOT)|(3,1)|(4,1)|\" \n",
    "        elif start_pos[1] == 2:\n",
    "            row2 = \"|(1,2)|(BOT)|(3,2)|LOSS-1|\"\n",
    "        elif start_pos[1] == 3:\n",
    "            row1 = \"|(1,3)|(BOT)|(3,3)|WIN+1|\"\n",
    "    if start_pos[0] == 3:\n",
    "        if start_pos[1] == 1:\n",
    "            row3 = \"|(1,1)|(2,1)|(BOT)|(4,1)|\" \n",
    "        elif start_pos[1] == 2:\n",
    "            row2 = \"|(1,2)|(WALL)|(BOT)|LOSS-1|\"\n",
    "        elif start_pos[1] == 3:\n",
    "            row1 = \"|(1,3)|(2,3)|(BOT)|WIN+1|\"\n",
    "    if start_pos[0] == 4:\n",
    "        if start_pos[1] == 1:\n",
    "            row3 = \"|(1,1)|(2,1)|(3,1)|(BOT)|\"\n",
    "        elif start_pos[1] == 2:\n",
    "            row2 = \"|(1,2)|(WALL)|(3,2)|LOSS-1|\"\n",
    "        elif start_pos[1] == 3:\n",
    "            row1 = \"|(1,3)|(2,3)|(3,3)|WIN+1|\"\n",
    "    \n",
    "    return (\"Generating Grid View: \\n  --------------------------------- \\n\" + row1 + \"\\n\" +  row2 + \"\\n\" + row3 + \"\\n  --------------------------------- \\n Movements:\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Examine the sample policy (in the cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifiable Coding from grid_world.py by Harry Goldingay\n",
    "\n",
    "# Sample policy summary: \n",
    "\n",
    "#|(1,3)|(2,3)|(3,3)|(4,3)| (3, x) cannot move up so move right.\n",
    "#|(1,2)|(2,2)|(3,2)|(4,2)| \n",
    "#|(1,1)|(2,1)|(3,1)|(4,1)| (4,1)  cannot move up, so move left.\n",
    "\n",
    "def sample_policy(pos):\n",
    "    #In the bottom right position, move left\n",
    "    if(pos == [4,1]): \n",
    "        return Move.left\n",
    "    #In the top row, move right\n",
    "    elif(pos[1] == 3):\n",
    "        return Move.right\n",
    "    #Otherwise, move up\n",
    "    else:\n",
    "        return Move.up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modifiable Coding from grid_world.py by Harry Goldingay\n",
    "\n",
    "''' Returns a random position of one of the following: (1,1), (1,2), (1,3), (2,1), (2,3), (3,1), (3,2), (3,3), (4,1), (4,2) (4,3)'''\n",
    "def random_pos():\n",
    "    pos = [random.randint(1,4), random.randint(1,3)]\n",
    "    if pos == [2,2] or pos == [4,2] or pos == [4,3]:\n",
    "        return random_pos()\n",
    "    else:\n",
    "        return pos\n",
    "    \n",
    "random_pos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don’t need to understand any of the implementation details, but should be familiar with:\n",
    "- Move: You will use this enum to specify a move in grid world. For instance, if you wanted to specify that an agent should move up in a given position, you would recommend the move Move.up.\n",
    "- random_pos: This returns a (valid) random position in grid world. A position is simply a two element array, with the first element being the x coordinate in the grid and the second being the y coordinate. As in the lecture slides, positions start at 1 (so the bottom right position would be [4,1]).\n",
    "- run_policy: Given a starting position and a policy, run policy generates a sample sequence of positions and rewards from that starting position under that policy. It returns two values: the first being an array of the positions visited and the second being the corresponding reward received in those positions.\n",
    "\n",
    "To generate a sequence, you will also need to define a policy. In this implementation, a policy is simply a function which takes a position as argument and returns a Move enum containing the policy’s recommended move in that position. I have defined a sample policy (sample_policy)\n",
    "as an example. The sample policy is suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "#### Step 4: Try running the policy and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Randomly generated starting position for robot is... [1, 2]\n",
      "\n",
      "Generating Grid View: \n",
      "  --------------------------------- \n",
      "|(1,3)|(2,3)|(3,3)|WIN+1|\n",
      "|(BOT)|(WALL)|(3,2)|LOSS-1|\n",
      "|(1,1)|(2,1)|(3,1)|(4,1)|\n",
      "  --------------------------------- \n",
      " Movements:\n",
      "[Step: 1] Robot attempted to  Move.up from [1, 2] to [1, 3] and succeeded.\n",
      "[Step: 2] Robot attempted to  Move.right from [1, 3] to [2, 3] and succeeded.\n",
      "[Step: 3] Robot attempted to  Move.right from [2, 3] to [3, 3] and succeeded.\n",
      "[Step: 4] Robot attempted to Move.up from [3, 3] to an out of bounds/illegal position and failed.\n",
      "[Step: 5] Robot attempted to  Move.right from [3, 3] to [4, 3] and succeeded.\n",
      "The robot landed in the terminal state of: [4, 3]\n",
      "\n",
      "============================== \n",
      " Grid World game has ended... \n",
      "============================== \n",
      "\n",
      "Results: \n",
      "\n",
      "The robot won, congratulations!\n",
      "Your robot worked out a path to the terminal state in 5 moves. \n",
      "Want to try again? \n",
      "\n",
      "Here are the positions and rewards that the robot attained along the way: \n",
      "\n",
      "Positions: [[1, 2], [1, 3], [2, 3], [3, 3], [3, 3], [4, 3]] || Rewards: [-0.04, -0.04, -0.04, -0.04, -0.04, 1]\n",
      "\n",
      "\n",
      "([[1, 2], [1, 3], [2, 3], [3, 3], [3, 3], [4, 3]], [-0.04, -0.04, -0.04, -0.04, -0.04, 1])\n"
     ]
    }
   ],
   "source": [
    "#Modifiable Coding from grid_world.py by Harry Goldingay\n",
    "\n",
    "#|(1,3)|(2,3)|(3,3)|(4,3)| <br>\n",
    "#|(1,2)|(2,2)|(3,2)|(4,2)| <br>\n",
    "#|(1,1)|(2,1)|(3,1)|(4,1)|\n",
    "\n",
    "sample = run_policy(random_pos(), sample_policy)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### Task 1: Calculate Discounted Utility\n",
    "_Write a function calculate_discounted_utility which takes two parameters: a sequence of rewards and a discount factor 𝛾 and returns the discounted utility of the reward sequence under that discount factor. Test your function by testing it with some sequences and values of 𝛾 which you have calculated by hand._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Translate the dicounted utility mathematical equation to a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Task 1 function '''\n",
    "def calculate_discounted_utility(seq_of_rewards, gamma):       # seq_of_rewards = sequence of rewards, gamme = discount\n",
    "    disc_utility = 0.0                                         # Let's set the initial value of discount utility...\n",
    "    for i, reward in enumerate(seq_of_rewards):\n",
    "        # discount_utility = discount_utility_previous + (reward x (gamma to the power of i))\n",
    "        disc_utility = disc_utility + (reward * (gamma ** i)) \n",
    "    return disc_utility\n",
    "\n",
    "''' Just a helper method that clearly prints out the sequence of rewards, gamme and discounted utility.'''\n",
    "def print_discounted(title, seq_of_rewards, discount):\n",
    "    print(\"------------------------\")\n",
    "    print(title + \": with rewards of \" + str(seq_of_rewards) + \" with a gamma of \" + str(discount))\n",
    "    print(\"Discounted Utility Value: \" + str(calculate_discounted_utility(seq_of_rewards, discount)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Check with hand-written values whether the function is correct..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Test 1: with rewards of [1.0, 1.0, 1.0] with a gamma of 1.0\n",
      "Discounted Utility Value: 3.0\n",
      "------------------------\n",
      "Test 2: with rewards of [1.0, 1.0, 1.0] with a gamma of 0.5\n",
      "Discounted Utility Value: 1.75\n",
      "------------------------\n",
      "Test 3: with rewards of [1.0, 1.0, 1.0] with a gamma of 0.25\n",
      "Discounted Utility Value: 1.3125\n",
      "------------------------\n",
      "Test 4: with rewards of [1.0, 0.0, 1.0] with a gamma of 0.3\n",
      "Discounted Utility Value: 1.09\n"
     ]
    }
   ],
   "source": [
    "# 1 1 1 gamma - 1 - value 3.0\n",
    "print_discounted(\"Test 1\", [1.0, 1.0, 1.0], 1.0)\n",
    "# 1 1 1 gamma - 0.5 - value 1.75\n",
    "print_discounted(\"Test 2\", [1.0, 1.0, 1.0], 0.5)\n",
    "# 1 1 1 gamma - 0.25 - value 1.3125\n",
    "print_discounted(\"Test 3\", [1.0, 1.0, 1.0], 0.25)\n",
    "# 1 0 1 gamma - 0.3 - value 1.09\n",
    "print_discounted(\"Test 4\", [1.0, 0.0, 1.0], 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "--------------\n",
    "### Task 2: Temporal Difference Learning\n",
    "_Use Python to implement temporal difference learning to estimate the expected utility of states of grid world under a given policy and discount factor._\n",
    "You will need to:\n",
    "- starting at a random state, sample a sequence for the given policy,\n",
    "- use the update equation discussed in lectures to set and update the expected utility of each state encountered in the sequence based on observed transitions and rewards,\n",
    "- iterate the above until differences between utility estimates become small.\n",
    "\n",
    "_Validate your implementation by running it against the optimal policy for grid world with a discount factor of 1. Does it match the values given in the lecture slides?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
